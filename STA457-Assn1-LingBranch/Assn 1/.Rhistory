#
# #Step 4: Run a regression model with all data to identify and remove insignificant variables
#
#
# #Creating the Final Model
# #Step 1: Create testing and training sets
# numsamples=dim()[1]
# sampling.rate=0.8
# training<-sample(1:numsamples,sample.rate*numsamples,replace=FALSE)
# trainingSet<-subset([training,])
# testing<-setdiff(1:numsamples,training)
# testingSet<-subset([testing,])
#
# #Step 2: Create a regression using the training set
# finalLogit<-glm(CKD~.,data=,family="binomial")
# summary(finalLogit)
#
# #Step 3: Measure the accuracy by applying the regression on the testing set and calculating the mean square error of the difference between the predicted and actual values.
# testingSet$model_prob<-predict(finalLogit,testingSet,type="response")
#Reading the Data
CKDData_raw<-read.csv("CKDDataSet.csv", head=TRUE)
#Cleaning the Data
#Step 1: Omit any missing data points
CKDData_v1<-na.omit(CKDData_raw)
#Step 2: Convert categorical data to binary variables
install.packages("fastDummies")
library("fastDummies")
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
#Step 3: Run a multicollinearity test and remove correlated variables
install.packages(car)
library(car)
CKDv2_logit<-glm(CKD~.,data=CKDData_v2,family="binomial")
CKDv2_logit$coefficients
#When I tried to run vif, it would not work because there was perfect collinearity
#If a coefficient is NA, that means there is collinearity between variables
#Thus, I displayed the coefficients, and noted that Total.Chol was NA
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
#I double checked if this eliminated all NA coefficients:
CKDv3_logit$coefficients
#Run the collinearity test. Each time I remove a variable, the collinearity will change.
#Thus, I need to run it iteratively through a while loop, where it will keep removing highly correlated variables
numofvif<-unname(vif(CKDv3_logit))
namesofvif<-names(vif(CKDv3_logit))
#Confirms if there is any collinearity present
absnumofvif<-abs(numofvif)
anycolinearity<-function(absnumofvif){
for(corr in absnumofvif){
if (corr>3){
return(TRUE)
}
}
return(FALSE)
}
#Remove all variables with collinearity of greater than 3
removeVarwithColl<-function(absnumofvif,namesofvif){
goodVar<-c()
for (i in 1:length(namesofvif)){
if(absnumofvif[i]>3){
next
}
else{
goodVar<-c(goodVar,namesofvif[i])
}
}
return(goodVar)
}
noncollinearity<-function(goodVar){
seen <- FALSE
new <- c()
for (var in goodVar){
if (length(goodVar[var]) > 6){
if (substr(goodVar[var], 1, 7) == "Racegrp"){
seen = TRUE
}
else{
new <- c(new, goodVar[var])
}
}
else{
new <- c(new, goodVar[var])
}
}
new <- c(new, "Racegrp")
print(new)
print(names(CKDData_raw))
CKDData_raw<-read.csv("CKDDataSet.csv",head=TRUE)
CKDData_raw <- CKDData_raw[,new]
print(CKDData_raw)
CKDData_v1<-na.omit(CKDData_raw)
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
numofvif<-unname(vif(CKDv3_logit))
return(numofvif)
}
goodVar <- removeVarwithColl(absnumofvif,namesofvif)
noncollinearity(goodVar)
#
# #Double check that this has eliminated all of the correlated variables
# vif(CKDv3_logit)
#
# #Step 4: Run a regression model with all data to identify and remove insignificant variables
#
#
# #Creating the Final Model
# #Step 1: Create testing and training sets
# numsamples=dim()[1]
# sampling.rate=0.8
# training<-sample(1:numsamples,sample.rate*numsamples,replace=FALSE)
# trainingSet<-subset([training,])
# testing<-setdiff(1:numsamples,training)
# testingSet<-subset([testing,])
#
# #Step 2: Create a regression using the training set
# finalLogit<-glm(CKD~.,data=,family="binomial")
# summary(finalLogit)
#
# #Step 3: Measure the accuracy by applying the regression on the testing set and calculating the mean square error of the difference between the predicted and actual values.
# testingSet$model_prob<-predict(finalLogit,testingSet,type="response")
install.packages("fastDummies")
#Reading the Data
CKDData_raw<-read.csv("CKDDataSet.csv", head=TRUE)
#Cleaning the Data
#Step 1: Omit any missing data points
CKDData_v1<-na.omit(CKDData_raw)
#Step 2: Convert categorical data to binary variables
install.packages("fastDummies")
library("fastDummies")
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
#Step 3: Run a multicollinearity test and remove correlated variables
install.packages(car)
library(car)
CKDv2_logit<-glm(CKD~.,data=CKDData_v2,family="binomial")
CKDv2_logit$coefficients
#When I tried to run vif, it would not work because there was perfect collinearity
#If a coefficient is NA, that means there is collinearity between variables
#Thus, I displayed the coefficients, and noted that Total.Chol was NA
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
#I double checked if this eliminated all NA coefficients:
CKDv3_logit$coefficients
#Run the collinearity test. Each time I remove a variable, the collinearity will change.
#Thus, I need to run it iteratively through a while loop, where it will keep removing highly correlated variables
numofvif<-unname(vif(CKDv3_logit))
namesofvif<-names(vif(CKDv3_logit))
#Confirms if there is any collinearity present
absnumofvif<-abs(numofvif)
anycolinearity<-function(absnumofvif){
for(corr in absnumofvif){
if (corr>3){
return(TRUE)
}
}
return(FALSE)
}
#Remove all variables with collinearity of greater than 3
removeVarwithColl<-function(absnumofvif,namesofvif){
goodVar<-c()
for (i in 1:length(namesofvif)){
if(absnumofvif[i]>3){
next
}
else{
goodVar<-c(goodVar,namesofvif[i])
}
}
return(goodVar)
}
noncollinearity<-function(goodVar){
seen <- FALSE
new <- c()
for (var in goodVar){
if (length(goodVar[var]) > 6){
if (substr(goodVar[var], 1, 7) == "Racegrp"){
seen = TRUE
}
else{
new <- c(new, goodVar[var])
}
}
else{
new <- c(new, goodVar[var])
}
}
new <- c(new, "Racegrp")
print(new)
print(names(CKDData_raw))
CKDData_raw<-read.csv("CKDDataSet.csv",head=TRUE)
CKDData_raw <- CKDData_raw[,new]
print(CKDData_raw)
CKDData_v1<-na.omit(CKDData_raw)
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
numofvif<-unname(vif(CKDv3_logit))
return(numofvif)
}
goodVar <- removeVarwithColl(absnumofvif,namesofvif)
noncollinearity(goodVar)
#
# #Double check that this has eliminated all of the correlated variables
# vif(CKDv3_logit)
#
# #Step 4: Run a regression model with all data to identify and remove insignificant variables
#
#
# #Creating the Final Model
# #Step 1: Create testing and training sets
# numsamples=dim()[1]
# sampling.rate=0.8
# training<-sample(1:numsamples,sample.rate*numsamples,replace=FALSE)
# trainingSet<-subset([training,])
# testing<-setdiff(1:numsamples,training)
# testingSet<-subset([testing,])
#
# #Step 2: Create a regression using the training set
# finalLogit<-glm(CKD~.,data=,family="binomial")
# summary(finalLogit)
#
# #Step 3: Measure the accuracy by applying the regression on the testing set and calculating the mean square error of the difference between the predicted and actual values.
# testingSet$model_prob<-predict(finalLogit,testingSet,type="response")
print(var)
if (length(goodVar[var]) > 6){
if (substr(goodVar[var], 1, 7) == "Racegrp"){
seen = TRUE
}
else{
new <- c(new, goodVar[var])
}
}
for (var in goodVar){
print(var)
if (length(goodVar[var]) > 6){
if (substr(goodVar[var], 1, 7) == "Racegrp"){
seen = TRUE
}
else{
new <- c(new, goodVar[var])
}
}
else{
new <- c(new, goodVar[var])
}
}
length("ID")
length("hello")
#Reading the Data
CKDData_raw<-read.csv("CKDDataSet.csv", head=TRUE)
#Cleaning the Data
#Step 1: Omit any missing data points
CKDData_v1<-na.omit(CKDData_raw)
#Step 2: Convert categorical data to binary variables
install.packages("fastDummies")
library("fastDummies")
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
#Step 3: Run a multicollinearity test and remove correlated variables
install.packages(car)
library(car)
CKDv2_logit<-glm(CKD~.,data=CKDData_v2,family="binomial")
CKDv2_logit$coefficients
#When I tried to run vif, it would not work because there was perfect collinearity
#If a coefficient is NA, that means there is collinearity between variables
#Thus, I displayed the coefficients, and noted that Total.Chol was NA
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
#I double checked if this eliminated all NA coefficients:
CKDv3_logit$coefficients
#Run the collinearity test. Each time I remove a variable, the collinearity will change.
#Thus, I need to run it iteratively through a while loop, where it will keep removing highly correlated variables
numofvif<-unname(vif(CKDv3_logit))
namesofvif<-names(vif(CKDv3_logit))
#Confirms if there is any collinearity present
absnumofvif<-abs(numofvif)
anycolinearity<-function(absnumofvif){
for(corr in absnumofvif){
if (corr>3){
return(TRUE)
}
}
return(FALSE)
}
#Remove all variables with collinearity of greater than 3
removeVarwithColl<-function(absnumofvif,namesofvif){
goodVar<-c()
for (i in 1:length(namesofvif)){
if(absnumofvif[i]>3){
next
}
else{
goodVar<-c(goodVar,namesofvif[i])
}
}
return(goodVar)
}
noncollinearity<-function(goodVar){
seen <- FALSE
new <- c()
for (var in goodVar){
if ((nchar(var) > 6)){
if (substr(var, 1, 7) == "Racegrp"){
seen = TRUE
}
else{
new <- c(new, var)
}
}
else{
new <- c(new, var)
}
}
new <- c(new, "Racegrp")
CKDData_raw<-read.csv("CKDDataSet.csv",head=TRUE)
CKDData_raw <- CKDData_raw[,new]
print(CKDData_raw)
CKDData_v1<-na.omit(CKDData_raw)
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
numofvif<-unname(vif(CKDv3_logit))
return(numofvif)
}
goodVar <- removeVarwithColl(absnumofvif,namesofvif)
noncollinearity(goodVar)
#
# #Double check that this has eliminated all of the correlated variables
# vif(CKDv3_logit)
#
# #Step 4: Run a regression model with all data to identify and remove insignificant variables
#
#
# #Creating the Final Model
# #Step 1: Create testing and training sets
# numsamples=dim()[1]
# sampling.rate=0.8
# training<-sample(1:numsamples,sample.rate*numsamples,replace=FALSE)
# trainingSet<-subset([training,])
# testing<-setdiff(1:numsamples,training)
# testingSet<-subset([testing,])
#
# #Step 2: Create a regression using the training set
# finalLogit<-glm(CKD~.,data=,family="binomial")
# summary(finalLogit)
#
# #Step 3: Measure the accuracy by applying the regression on the testing set and calculating the mean square error of the difference between the predicted and actual values.
# testingSet$model_prob<-predict(finalLogit,testingSet,type="response")
install.packages("fastDummies")
#Reading the Data
CKDData_raw<-read.csv("CKDDataSet.csv", head=TRUE)
#Cleaning the Data
#Step 1: Omit any missing data points
CKDData_v1<-na.omit(CKDData_raw)
#Step 2: Convert categorical data to binary variables
install.packages("fastDummies")
library("fastDummies")
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
#Step 3: Run a multicollinearity test and remove correlated variables
install.packages(car)
library(car)
CKDv2_logit<-glm(CKD~.,data=CKDData_v2,family="binomial")
CKDv2_logit$coefficients
#When I tried to run vif, it would not work because there was perfect collinearity
#If a coefficient is NA, that means there is collinearity between variables
#Thus, I displayed the coefficients, and noted that Total.Chol was NA
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
#I double checked if this eliminated all NA coefficients:
CKDv3_logit$coefficients
#Run the collinearity test. Each time I remove a variable, the collinearity will change.
#Thus, I need to run it iteratively through a while loop, where it will keep removing highly correlated variables
numofvif<-unname(vif(CKDv3_logit))
namesofvif<-names(vif(CKDv3_logit))
#Confirms if there is any collinearity present
absnumofvif<-abs(numofvif)
anycolinearity<-function(absnumofvif){
for(corr in absnumofvif){
if (corr>3){
return(TRUE)
}
}
return(FALSE)
}
#Remove all variables with collinearity of greater than 3
removeVarwithColl<-function(absnumofvif,namesofvif){
goodVar<-c()
for (i in 1:length(namesofvif)){
if(absnumofvif[i]>3){
next
}
else{
goodVar<-c(goodVar,namesofvif[i])
}
}
return(goodVar)
}
noncollinearity<-function(goodVar){
seen <- FALSE
new <- c()
for (var in goodVar){
if ((nchar(var) > 6)){
if (substr(var, 1, 7) == "Racegrp"){
seen = TRUE
}
else{
new <- c(new, var)
}
}
else{
new <- c(new, var)
}
}
new <- c(new, "Racegrp")
CKDData_raw<-read.csv("CKDDataSet.csv",head=TRUE)
CKDData_raw <- CKDData_raw[,new]
print(CKDData_raw)
CKDData_v1<-na.omit(CKDData_raw)
CKDData_v2<-dummy_cols(CKDData_v1,remove_first_dummy = TRUE)
CKDData_v2$Racegrp<-NULL
CKDData_v2$CareSource<-NULL
CKDData_v2$`CareSource_ `<-NULL
CKDData_v3<-CKDData_v2
CKDData_v3$Total.Chol<-NULL
CKDv3_logit<-glm(CKD~.,data=CKDData_v3,family="binomial")
numofvif<-unname(vif(CKDv3_logit))
return(numofvif)
}
goodVar <- removeVarwithColl(absnumofvif,namesofvif)
noncollinearity(goodVar)
#
# #Double check that this has eliminated all of the correlated variables
# vif(CKDv3_logit)
#
# #Step 4: Run a regression model with all data to identify and remove insignificant variables
#
#
# #Creating the Final Model
# #Step 1: Create testing and training sets
# numsamples=dim()[1]
# sampling.rate=0.8
# training<-sample(1:numsamples,sample.rate*numsamples,replace=FALSE)
# trainingSet<-subset([training,])
# testing<-setdiff(1:numsamples,training)
# testingSet<-subset([testing,])
#
# #Step 2: Create a regression using the training set
# finalLogit<-glm(CKD~.,data=,family="binomial")
# summary(finalLogit)
#
# #Step 3: Measure the accuracy by applying the regression on the testing set and calculating the mean square error of the difference between the predicted and actual values.
# testingSet$model_prob<-predict(finalLogit,testingSet,type="response")
print(names(CKDData_raw))
print(new)
print(var)
print(var)
if ((nchar(var) > 6)){
if (substr(var, 1, 7) == "Racegrp"){
seen = TRUE
}
else{
new <- c(new, var)
}
}
else{
new <- c(new, var)
}
print(var)
getwd
getwd()
dataDir <- "/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1/DJ Data Monthly"
currDir <- "/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1"
monthlydata <- getAdC(dataDir, currDir)
unlink('~/GitHub/STA457-Assn1/Assn 1/Assn1_STA457_cache', recursive = TRUE)
this.dir <- dirname(parent.frame(2)$ofile)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
setwd("C:/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1)
setwd("C:/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1")
setwd(C:/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1)
setwd(/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1)
setwd(Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1)
setwd("Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1")
setwd("C:/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1")
setwd("~/GitHub/STA457-Assn1/Assn 1")
setwd("C:/Users/Ling Long/Documents/GitHub/Assignments/STA457-Assn1/Assn 1")
